{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feb30a31-fec2-4801-a0b2-f414fc4eedc5",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a961ab1f-e4c6-422f-9533-0641a7296ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/kategoribert-lr-2e-05-maxtoken-128-epochs-8-bs-32'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_TOKEN_LEN = 128\n",
    "LEARNING_RATE = 2e-5 \n",
    "NUM_EPOCHS = 8\n",
    "BATCH_SIZE = 32\n",
    "MODEL_NAME = f\"models/kategoribert-lr-{LEARNING_RATE}-maxtoken-{MAX_TOKEN_LEN}-epochs-{NUM_EPOCHS}-bs-{BATCH_SIZE}\"\n",
    "MODEL_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d605e49c-6d43-4da4-b07a-9ae2351731a6",
   "metadata": {},
   "source": [
    "# Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfc0aefa-4069-4ca4-a2ae-729255fac23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b8b7500-a57b-48b3-b816-58a98eee8c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2798d3-85de-4f4c-a2b5-14ccc5efb3ee",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d76134b-bc81-40c0-87b6-84c415569994",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e3404c7-1849-43f6-9378-241c9d57b2ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da7d0599f788468f89d55e73ee237faf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/3.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset wiki_kategori/default (download: 24.93 MiB, generated: 25.82 MiB, post-processed: Unknown size, total: 50.75 MiB) to /home/jupyter/.cache/huggingface/datasets/Johannesemme___wiki_kategori/default/1.1.0/d1af97ac4d53f3037dfa4071a662c627516dcf691eed74e25f3b9a523c425c9b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "244b76764c194771910949e0e0a269bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c4da7a1d5d341199d4080d66d3d26ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/7.88M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c70a6414bbd40e1bd8a7988cb630137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/965k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdab96acf1894db2ac83da8b4d8d4985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33bb8f6e919b4683a8b43a47e87fc6ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1d42374f1a54936a0aa127b82c9c76c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/8522 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d5188f3bf144212b2961b864ff867d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/947 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44af7b7712194aacb5de66bba05f8f18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1052 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset wiki_kategori downloaded and prepared to /home/jupyter/.cache/huggingface/datasets/Johannesemme___wiki_kategori/default/1.1.0/d1af97ac4d53f3037dfa4071a662c627516dcf691eed74e25f3b9a523c425c9b. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85a07aa8108e4567a135ce10c6a07165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wikicat = load_dataset(\"Johannesemme/wiki_kategori\", download_mode='force_redownload')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d651b2a-0ea6-4eec-b051-17407f7ba772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Text', 'Title', 'Labels'],\n",
       "        num_rows: 8522\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Text', 'Title', 'Labels'],\n",
       "        num_rows: 947\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Text', 'Title', 'Labels'],\n",
       "        num_rows: 1052\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikicat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5420b791-39ce-4a0d-b02b-ad75fe708743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataset to pandas dataframe\n",
    "df = wikicat['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91aef256-4558-43b2-b85f-5ada050542a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Title</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Forbøn er at bede for andre end sig selv. En p...</td>\n",
       "      <td>Forbøn</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mad er en essentiel energikilde for mennesker,...</td>\n",
       "      <td>Mad</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Latinskolerne blev i de fleste købstæder opret...</td>\n",
       "      <td>Latinskole</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Galgenfrist betød oprindelig et kort tidsrum, ...</td>\n",
       "      <td>Galgenfrist</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Briller er betegnelsen for et synsforbedrende ...</td>\n",
       "      <td>Briller</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text        Title  \\\n",
       "0  Forbøn er at bede for andre end sig selv. En p...       Forbøn   \n",
       "1  Mad er en essentiel energikilde for mennesker,...          Mad   \n",
       "2  Latinskolerne blev i de fleste købstæder opret...   Latinskole   \n",
       "3  Galgenfrist betød oprindelig et kort tidsrum, ...  Galgenfrist   \n",
       "4  Briller er betegnelsen for et synsforbedrende ...      Briller   \n",
       "\n",
       "                                       Labels  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]  \n",
       "1  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "2  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "3  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "4  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Overview of data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "957b52a5-a65c-4d8d-8b3d-1eca595af62f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Uddannelse',\n",
       " 1: 'Samfund',\n",
       " 2: 'Videnskab',\n",
       " 3: 'Natur',\n",
       " 4: 'Teknologi',\n",
       " 5: 'Kultur',\n",
       " 6: 'Historie',\n",
       " 7: 'Sundhed',\n",
       " 8: 'Geografi',\n",
       " 9: 'Økonomi',\n",
       " 10: 'Sport',\n",
       " 11: 'Religion',\n",
       " 12: 'Politik',\n",
       " 13: 'Erhvervsliv'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names = wikicat[\"train\"].features[\"Labels\"].feature.names\n",
    "id2label = {idx:label for idx, label in enumerate(label_names)}\n",
    "label2id = {label:idx for idx, label in enumerate(label_names)}\n",
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69eb2b95-6f12-4d0f-bbcd-30bb5621c123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter,OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "895a5df6-689d-44d4-86d6-5771b9817ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_label_list = []\n",
    "for ix, row in df.iterrows():\n",
    "    list_ = np.array(row[\"Labels\"])\n",
    "    vals = np.where(list_ == 1)[0]\n",
    "    total_label_list.extend(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "842b04e1-e9d8-47e5-8874-4e3839c041fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.6988, 0.3708, 1.0630, 1.4064, 3.0649, 0.4606, 0.9406, 1.8765, 2.7549,\n",
       "        0.9081, 1.8298, 0.8246, 0.8523, 1.7472], device='cuda:0',\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = dict(OrderedDict(sorted(Counter(total_label_list).items())))\n",
    "w = np.array(list(d.values())).sum() / (14*np.array(list(d.values())))\n",
    "class_weights = torch.tensor(w).to(device)\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148de792-ffdc-4cab-9cdd-6e9c4aec4a54",
   "metadata": {},
   "source": [
    "# Tokenize and encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "841c35c6-f27c-4d72-bb62-aaa885aa3f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = \"Maltehb/danish-bert-botxo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f86aae8b-b749-4cf2-96da-fc9389b190a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3fa7d5c-e853-4454-9247-560887a3fcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e203eaa9-a4a2-4769-b4af-7c8cc59f2179",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(batch):\n",
    "    text = batch[\"Text\"]\n",
    "    encoding = tokenizer(text, padding=True, truncation=True, max_length=MAX_TOKEN_LEN) \n",
    "    # Create numpy array of shape (batch_size, num_labels)\n",
    "    labels_matrix = np.zeros((len(text), len(label_names)))\n",
    "    # Fill labels_matrix with indices of the text labels\n",
    "    for idx, row in enumerate(batch[\"Labels\"]):\n",
    "        labels_matrix[idx,:] = np.array([float(x) for x in row])\n",
    "    encoding[\"labels\"] = labels_matrix.tolist()\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80e31fb6-efa9-4859-86ea-d5885512161e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9e04424d71d44f98be1430e96b8db39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24031a99a87047da9ac04124d6f69216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6fae54f2a834ee9bd60d4e55cd6e00b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_dataset = wikicat.map(preprocess_data, batched=True, batch_size=None, remove_columns=wikicat['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76f2c2e6-2925-4b97-ae7c-0c675306adc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all to torch\n",
    "encoded_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f48e833-1866-4cca-af94-b90232b8ac07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_ids', 'token_type_ids', 'attention_mask', 'labels']\n"
     ]
    }
   ],
   "source": [
    "# Check dataset columns\n",
    "print(encoded_dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7a0cc8e-c846-4822-834e-1da9044a152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve an example\n",
    "example = encoded_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aeeb7a1f-bd28-4bde-90c6-046a9ef3e1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   2,  438,  222,   33,   39, 4742,   30,  394,  292,  176,  250,  771,\n",
      "          38,  493,   59, 5407,   30,   38,  617,  151])\n",
      "['[CLS]', 'forb', '##øn', 'er', 'at', 'bede', 'for', 'andre', 'end', 'sig', 'selv', '.', 'en', 'person', 'der', 'beder', 'for', 'en', 'anden', 'eller']\n"
     ]
    }
   ],
   "source": [
    "# Get token ids + the result of applying id2token\n",
    "print(example[\"input_ids\"][:20])\n",
    "print(tokenizer.convert_ids_to_tokens(example[\"input_ids\"])[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a94b64d-560b-4416-899c-b2820ce2bdf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] forbøn er at bede for andre end sig selv. en person der beder for en anden eller noget andet end sig selv betegnes som en forbeder. forbøn har en stor rolle i mange af verdens religioner, herunder også kristendommen. forbønnen findes både i de protestantiske kirker, den katolske kirke og den ortodokse kirke ; i den ortodokse kirke kaldes forbøn for ekteni. tanken med bønnen er et forsøg på at ændre noget i den usynlige verden, som derefter vil vise sig i den synlige. trods forbønnens store udbredelse blandt mange forskellige religioner og trosretninger, er der intet empirisk bevis for [SEP]'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use decoder to check how much text we actually process\n",
    "tokenizer.decode(example[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c69bfa8-5f0a-4fe3-b78a-2b907f052968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that labels are float - import because of the way BCE loss is defined\n",
    "example[\"labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a29d0c-77b9-4043-9854-728cae5194f9",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20a98497-f813-40fc-bfdd-879d60351c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a6c363a-7986-4ac4-bbc5-28aa27c25c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Maltehb/danish-bert-botxo were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Maltehb/danish-bert-botxo and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, \n",
    "                                                           problem_type=\"multi_label_classification\", \n",
    "                                                           num_labels=len(label_names),\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7aa7c1f5-bb64-47c8-a970-6cd26d5faae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"Maltehb/danish-bert-botxo\",\n",
       "  \"architectures\": [\n",
       "    \"BertForPreTraining\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"directionality\": \"bidi\",\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"Uddannelse\",\n",
       "    \"1\": \"Samfund\",\n",
       "    \"2\": \"Videnskab\",\n",
       "    \"3\": \"Natur\",\n",
       "    \"4\": \"Teknologi\",\n",
       "    \"5\": \"Kultur\",\n",
       "    \"6\": \"Historie\",\n",
       "    \"7\": \"Sundhed\",\n",
       "    \"8\": \"Geografi\",\n",
       "    \"9\": \"\\u00d8konomi\",\n",
       "    \"10\": \"Sport\",\n",
       "    \"11\": \"Religion\",\n",
       "    \"12\": \"Politik\",\n",
       "    \"13\": \"Erhvervsliv\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"Erhvervsliv\": 13,\n",
       "    \"Geografi\": 8,\n",
       "    \"Historie\": 6,\n",
       "    \"Kultur\": 5,\n",
       "    \"Natur\": 3,\n",
       "    \"Politik\": 12,\n",
       "    \"Religion\": 11,\n",
       "    \"Samfund\": 1,\n",
       "    \"Sport\": 10,\n",
       "    \"Sundhed\": 7,\n",
       "    \"Teknologi\": 4,\n",
       "    \"Uddannelse\": 0,\n",
       "    \"Videnskab\": 2,\n",
       "    \"\\u00d8konomi\": 9\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pooler_fc_size\": 768,\n",
       "  \"pooler_num_attention_heads\": 12,\n",
       "  \"pooler_num_fc_layers\": 3,\n",
       "  \"pooler_size_per_head\": 128,\n",
       "  \"pooler_type\": \"first_token_transform\",\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"problem_type\": \"multi_label_classification\",\n",
       "  \"transformers_version\": \"4.22.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6554f9f4-9ad6-4b64-b342-919b592d8bb1",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d12a0711-da40-486f-883c-b3e462a5948c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from transformers import EvalPrediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "349eaa6d-4f5e-447b-a31f-635675a485e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2daaf762-502b-4a21-91c0-f592e8ff0f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=labels)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a69504-2685-475d-a965-c83a40813017",
   "metadata": {},
   "source": [
    "# Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a147b2f1-6e70-401f-8e44-478966d18c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "266"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# configure logging so we see training loss\n",
    "logging_steps = len(encoded_dataset[\"train\"]) // BATCH_SIZE\n",
    "logging_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cc09fa71-4424-4c29-a8c6-838c090893c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6f8948e3-45f9-451e-85c7-7f7a3353ae9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_NAME,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=logging_steps,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a0f46e96-31d4-4c0f-ac12-33771e0b6aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from transformers import Trainer\n",
    "\n",
    "class MyTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        labels = labels.float()\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get('logits')\n",
    "        # compute custom loss\n",
    "        loss_fct = nn.BCEWithLogitsLoss(weight = class_weights)\n",
    "        loss = loss_fct(logits, labels.float())\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a5e6aa0e-5622-478e-ba47-760021415724",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = MyTrainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e785e2bd-cc74-41e2-9266-2b682c906975",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 947\n",
      "  Batch size = 32\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 03:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.0801347494125366,\n",
       " 'eval_f1': 0.1434878587196468,\n",
       " 'eval_roc_auc': 0.4805361102718602,\n",
       " 'eval_accuracy': 0.0,\n",
       " 'eval_runtime': 6.505,\n",
       " 'eval_samples_per_second': 145.58,\n",
       " 'eval_steps_per_second': 4.612}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af552583-4f07-4154-8d91-bacbdffd751b",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "54a43e0e-3ac8-4f73-bd6c-00201f14d5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 8522\n",
      "  Num Epochs = 8\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2136\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2136' max='2136' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2136/2136 26:42, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.304500</td>\n",
       "      <td>0.196714</td>\n",
       "      <td>0.349076</td>\n",
       "      <td>0.608013</td>\n",
       "      <td>0.210137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.172400</td>\n",
       "      <td>0.157608</td>\n",
       "      <td>0.570334</td>\n",
       "      <td>0.718535</td>\n",
       "      <td>0.419219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.141314</td>\n",
       "      <td>0.624211</td>\n",
       "      <td>0.749528</td>\n",
       "      <td>0.488912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.104500</td>\n",
       "      <td>0.141226</td>\n",
       "      <td>0.675073</td>\n",
       "      <td>0.791486</td>\n",
       "      <td>0.536431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.087000</td>\n",
       "      <td>0.139501</td>\n",
       "      <td>0.669612</td>\n",
       "      <td>0.785906</td>\n",
       "      <td>0.537487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.074800</td>\n",
       "      <td>0.140659</td>\n",
       "      <td>0.678970</td>\n",
       "      <td>0.793027</td>\n",
       "      <td>0.553326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.067400</td>\n",
       "      <td>0.141447</td>\n",
       "      <td>0.684008</td>\n",
       "      <td>0.797521</td>\n",
       "      <td>0.550158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.062200</td>\n",
       "      <td>0.141775</td>\n",
       "      <td>0.682555</td>\n",
       "      <td>0.799446</td>\n",
       "      <td>0.550158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 947\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to models/kategoribert-lr-2e-05-maxtoken-128-epochs-8-bs-32/checkpoint-267\n",
      "Configuration saved in models/kategoribert-lr-2e-05-maxtoken-128-epochs-8-bs-32/checkpoint-267/config.json\n",
      "Model weights saved in models/kategoribert-lr-2e-05-maxtoken-128-epochs-8-bs-32/checkpoint-267/pytorch_model.bin\n",
      "tokenizer config file saved in models/kategoribert-lr-2e-05-maxtoken-128-epochs-8-bs-32/checkpoint-267/tokenizer_config.json\n",
      "Special tokens file saved in models/kategoribert-lr-2e-05-maxtoken-128-epochs-8-bs-32/checkpoint-267/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 947\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to models/kategoribert-lr-2e-05-maxtoken-128-epochs-8-bs-32/checkpoint-534\n",
      "Configuration saved in models/kategoribert-lr-2e-05-maxtoken-128-epochs-8-bs-32/checkpoint-534/config.json\n",
      "Model weights saved in models/kategoribert-lr-2e-05-maxtoken-128-epochs-8-bs-32/checkpoint-534/pytorch_model.bin\n",
      "tokenizer config file saved in models/kategoribert-lr-2e-05-maxtoken-128-epochs-8-bs-32/checkpoint-534/tokenizer_config.json\n",
      "Special tokens file saved in models/kategoribert-lr-2e-05-maxtoken-128-epochs-8-bs-32/checkpoint-534/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 947\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to models/kategoribert-lr-2e-05-maxtoken-128-epochs-8-bs-32/checkpoint-801\n",
      "Configuration saved in models/kategoribert-lr-2e-05-maxtoken-128-epochs-8-bs-32/checkpoint-801/config.json\n",
      "Model weights saved in models/kategoribert-lr-2e-05-maxtoken-128-epochs-8-bs-32/checkpoint-801/pytorch_model.bin\n",
      "tokenizer config file saved in models/kategoribert-lr-2e-05-maxtoken-128-epochs-8-bs-32/checkpoint-801/tokenizer_config.json\n",
      "Special tokens file saved in models/kategoribert-lr-2e-05-maxtoken-128-epochs-8-bs-32/checkpoint-801/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 947\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to models/kategoribert-lr-2e-05-maxtoken-128-epochs-8-bs-32/checkpoint-1068\n",
      "Configuration saved in models/kategoribert-lr-2e-05-maxtoken-128-epochs-8-bs-32/checkpoint-1068/config.json\n",
      "Model weights saved in models/kategoribert-lr-2e-05-maxtoken-128-epochs-8-bs-32/checkpoint-1068/pytorch_model.bin\n",
      "tokenizer config file saved in models/kategoribert-lr-2e-05-maxtoken-128-epochs-8-bs-32/checkpoint-1068/tokenizer_config.json\n",
      "Special tokens file saved in models/kategoribert-lr-2e-05-maxtoken-128-epochs-8-bs-32/checkpoint-1068/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 947\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to models/kategoribert-lr-2e-05-maxtoken-128-epochs-8-bs-32/checkpoint-1335\n",
      "Configuration saved in models/kategoribert-lr-2e-05-maxtoken-128-epochs-8-bs-32/checkpoint-1335/config.json\n",
      "Model weights saved in models/kategoribert-lr-2e-05-maxtoken-128-epochs-8-bs-32/checkpoint-1335/pytorch_model.bin\n",
      "tokenizer config file saved in models/kategoribert-lr-2e-05-maxtoken-128-epochs-8-bs-32/checkpoint-1335/tokenizer_config.json\n",
      "Special tokens file saved in models/kategoribert-lr-2e-05-maxtoken-128-epochs-8-bs-32/checkpoint-1335/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 947\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to models/kategoribert-lr-2e-05-maxtoken-128-epochs-8-bs-32/checkpoint-1602\n",
      "Configuration saved in models/kategoribert-lr-2e-05-maxtoken-128-epochs-8-bs-32/checkpoint-1602/config.json\n",
      "Model weights saved in models/kategoribert-lr-2e-05-maxtoken-128-epochs-8-bs-32/checkpoint-1602/pytorch_model.bin\n",
      "tokenizer config file saved in models/kategoribert-lr-2e-05-maxtoken-128-epochs-8-bs-32/checkpoint-1602/tokenizer_config.json\n",
      "Special tokens file saved in models/kategoribert-lr-2e-05-maxtoken-128-epochs-8-bs-32/checkpoint-1602/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 947\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to models/kategoribert-lr-2e-05-maxtoken-128-epochs-8-bs-32/checkpoint-1869\n",
      "Configuration saved in models/kategoribert-lr-2e-05-maxtoken-128-epochs-8-bs-32/checkpoint-1869/config.json\n",
      "Model weights saved in models/kategoribert-lr-2e-05-maxtoken-128-epochs-8-bs-32/checkpoint-1869/pytorch_model.bin\n",
      "tokenizer config file saved in models/kategoribert-lr-2e-05-maxtoken-128-epochs-8-bs-32/checkpoint-1869/tokenizer_config.json\n",
      "Special tokens file saved in models/kategoribert-lr-2e-05-maxtoken-128-epochs-8-bs-32/checkpoint-1869/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 947\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to models/kategoribert-lr-2e-05-maxtoken-128-epochs-8-bs-32/checkpoint-2136\n",
      "Configuration saved in models/kategoribert-lr-2e-05-maxtoken-128-epochs-8-bs-32/checkpoint-2136/config.json\n",
      "Model weights saved in models/kategoribert-lr-2e-05-maxtoken-128-epochs-8-bs-32/checkpoint-2136/pytorch_model.bin\n",
      "tokenizer config file saved in models/kategoribert-lr-2e-05-maxtoken-128-epochs-8-bs-32/checkpoint-2136/tokenizer_config.json\n",
      "Special tokens file saved in models/kategoribert-lr-2e-05-maxtoken-128-epochs-8-bs-32/checkpoint-2136/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from models/kategoribert-lr-2e-05-maxtoken-128-epochs-8-bs-32/checkpoint-1869 (score: 0.6840077071290943).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2136, training_loss=0.12519889475589388, metrics={'train_runtime': 1603.6435, 'train_samples_per_second': 42.513, 'train_steps_per_second': 1.332, 'total_flos': 4484947997958144.0, 'train_loss': 0.12519889475589388, 'epoch': 8.0})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98e8d53-e559-49b4-836d-bcbf8cc7f44a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluate model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e3b9ab59-a989-45d6-8982-4b77b51a7250",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 1052\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.14116135239601135,\n",
       " 'test_f1': 0.6775907883082373,\n",
       " 'test_roc_auc': 0.7947170417516027,\n",
       " 'test_accuracy': 0.5484790874524715,\n",
       " 'test_runtime': 8.2314,\n",
       " 'test_samples_per_second': 127.803,\n",
       " 'test_steps_per_second': 4.009}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_output = trainer.predict(encoded_dataset[\"test\"])\n",
    "preds_output.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43dcf27-65d5-46cb-8b88-01b5b365b6bd",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f37db3aa-77b0-4789-968d-350caeb46b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicted_labels(test_sentence):\n",
    "    encoding = encoding = tokenizer(test_sentence, padding=True, truncation=True, max_length=MAX_TOKEN_LEN, return_tensors=\"pt\") \n",
    "    encoding = {k:v.to(device) for k,v in encoding.items() if k in [\"input_ids\", \"attention_mask\"]}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = trainer.model(**encoding)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # apply sigmoid + threshold\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(logits.squeeze().cpu())\n",
    "    predictions = np.zeros(probs.shape)\n",
    "\n",
    "    predictions[np.where(probs >= 0.5)] = 1\n",
    "    # turn predicted id's into actual label names\n",
    "    predicted_labels = [id2label[idx] for idx, label in enumerate(predictions) if label == 1.0]\n",
    "    return predicted_labels, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8d52ebbe-ef73-4311-9656-eafb619eee32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Uddannelse'],\n",
       " tensor([0.7858, 0.1520, 0.0479, 0.0289, 0.0154, 0.0378, 0.0259, 0.0158, 0.0114,\n",
       "         0.0748, 0.0266, 0.0204, 0.0498, 0.0134]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = \"Visse betalingskort, mobilbanker, netbanker og pengeautomater er nede\"\n",
    "test_sentence = \"Det er godt at spise mange gulerødder, da disse er høje på c vitaminer\"\n",
    "test_sentence = \"I weekenden er VM i ridesport kommet rigtig godt i gang, og de danske ryttere er kommet utroligt godt fra start. Danmark sikrede sig en guldmedalje\"\n",
    "test_sentence = \"På søndag er der gudstjeneste i Allerød\"\n",
    "test_sentence = \"På kunstmuseet Arken kan man opleve mange spændende udstillinger\"\n",
    "test_sentence = \"Flere uddannelser mangler studerende\"\n",
    "\n",
    "predicted_labels(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b3e2711e-4adc-4af3-b024-49c2505c3e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Uddannelse',\n",
       " 'Samfund',\n",
       " 'Videnskab',\n",
       " 'Natur',\n",
       " 'Teknologi',\n",
       " 'Kultur',\n",
       " 'Historie',\n",
       " 'Sundhed',\n",
       " 'Geografi',\n",
       " 'Økonomi',\n",
       " 'Sport',\n",
       " 'Religion',\n",
       " 'Politik',\n",
       " 'Erhvervsliv']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0357e699-69c1-4c1c-8ba7-425e29b0d004",
   "metadata": {},
   "source": [
    "# Save best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0db630ef-a0da-4ac3-8c3b-9a8ce4f49707",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../HF/kategoriBERT\n",
      "Configuration saved in ../HF/kategoriBERT/config.json\n",
      "Model weights saved in ../HF/kategoriBERT/pytorch_model.bin\n",
      "tokenizer config file saved in ../HF/kategoriBERT/tokenizer_config.json\n",
      "Special tokens file saved in ../HF/kategoriBERT/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "# Save best model \n",
    "trainer.save_model(\"../HF/kategoriBERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe69fd7-d5c4-482d-aab7-2bd9cfcc45db",
   "metadata": {},
   "source": [
    "# Remove checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283a19a9-2c46-401d-b8dd-d7fcafd3d5fe",
   "metadata": {},
   "source": [
    "Since the checkpoints use storage and we do not need these we delete them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "88901505-68e0-4288-9725-bb35350d6880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f680f8cd-7e16-4a38-ae76-f48db12a7913",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m96",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m96"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
